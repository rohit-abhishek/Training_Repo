{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab670ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# For every model the token generated are little different. Please check gpt tokenizer web portal \n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n",
    "tokens = encoding.encode(\"Hi my name is Rohit and I like Saag aloo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8045d10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12194, 922, 1308, 382, 65416, 278, 326, 357, 1299, 8455, 348, 434, 3782]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0adaf",
   "metadata": {},
   "source": [
    "``` \n",
    "Notice Saag aloo got 4 different token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30514972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12194 = Hi\n",
      "922 =  my\n",
      "1308 =  name\n",
      "382 =  is\n",
      "65416 =  Roh\n",
      "278 = it\n",
      "326 =  and\n",
      "357 =  I\n",
      "1299 =  like\n",
      "8455 =  Sa\n",
      "348 = ag\n",
      "434 =  al\n",
      "3782 = oo\n"
     ]
    }
   ],
   "source": [
    "# let's decode this \n",
    "for tid in tokens:\n",
    "    t_text=encoding.decode([tid])\n",
    "    print(f\"{tid} = {t_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f35b05",
   "metadata": {},
   "source": [
    "# Illusion of memory\n",
    "Every API call to LLM is stateless i.e. it does not have previous data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79306e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key and api_key.startswith(\"sk-proj\"): \n",
    "    pass\n",
    "else: \n",
    "    print(\"OpenAI key not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25be1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\":\"system\", \"content\": \"You are helpful assistant\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Hi! I am Rohit Abhishek\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455bd45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Rohit Abhishek! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI \n",
    "openai=OpenAI() \n",
    "response=openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e53c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m sorry, but I don’t know your name. Could you please tell me?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "\n",
    "response=openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0575e",
   "metadata": {},
   "source": [
    "``` \n",
    "As LLMs are unable to maintain state - the LLM doesnt know my name now. It is a fresh run.. But if we had provided the earlier response along then we had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd8c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Rohit Abhishek!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Rohit Abhishek! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e4a7f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Rohit Abhishek. How can I help you further?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c4c29",
   "metadata": {},
   "source": [
    "## To recap\n",
    "\n",
    "With apologies if this is obvious to you - but it's still good to reinforce:\n",
    "\n",
    "1. Every call to an LLM is stateless\n",
    "2. We pass in the entire conversation so far in the input prompt, every time\n",
    "3. This gives the illusion that the LLM has memory - it apparently keeps the context of the conversation\n",
    "4. But this is a trick; it's a by-product of providing the entire conversation, every time\n",
    "5. An LLM just predicts the most likely next tokens in the sequence; if that sequence contains \"My name is Ed\" and later \"What's my name?\" then it will predict.. Ed!\n",
    "\n",
    "The ChatGPT product uses exactly this trick - every time you send a message, it's the entire conversation that gets passed in.\n",
    "\n",
    "\"Does that mean we have to pay extra each time for all the conversation so far\"\n",
    "\n",
    "For sure it does. And that's what we WANT. We want the LLM to predict the next tokens in the sequence, looking back on the entire conversation. We want that compute to happen, so we need to pay the electricity bill for it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac3991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
