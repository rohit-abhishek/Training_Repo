{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d89c666",
   "metadata": {},
   "source": [
    "# Use case for using RAG - Creating Knowledge Base - II - Creating vectors\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by employees of Insurellm, an Insurance Tech company\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2670df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv \n",
    "import glob \n",
    "import gradio as gr \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a1609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# including langchain imports \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader          # Directory loader will load whole directory and text loader will load whole document\n",
    "from langchain_text_splitters import CharacterTextSplitter                   # splitting the content in chunks so that there is some meaningful context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more imports \n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# lets add one more embedding model from local mxbai to compare the output\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 llm model for gradio screen \n",
    "OLLAMA_MODEL=\"llama3.1\"\n",
    "OPENAI_MODEL=\"gpt-4o-mini\"\n",
    "\n",
    "db_name_ollama=\"vector_db_ollama_mxbai\"\n",
    "db_name_openai=\"vector_db_openai_embed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 instances one frontier model and another is local model\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "openai=OpenAI()\n",
    "ollama=OpenAI(base_url=os.getenv(\"OLLAMA_BASE_URL\"), api_key=os.getenv(\"OLLAMA_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca3517",
   "metadata": {},
   "source": [
    "### 1. Now grab documents and load them to Langchain Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f75117",
   "metadata": {},
   "outputs": [],
   "source": [
    "context={} \n",
    "\n",
    "# grab the documents in knowledge-base all folders \n",
    "folders=glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "text_loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "\n",
    "documents=[] \n",
    "for folder in folders: \n",
    "\n",
    "    # grab the name of the file in sub-folder name e.g. products, employees etc\n",
    "    doc_type=os.path.basename(folder)\n",
    "\n",
    "    # load the files from the directory \n",
    "    loader=DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs=loader.load()\n",
    "\n",
    "    # for each folder document loaded add a metadata tag\n",
    "    for doc in folder_docs: \n",
    "        doc.metadata[\"doc_type\"]=doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af67f53",
   "metadata": {},
   "source": [
    "### Split the documents to manageable chunks \n",
    "\n",
    "if chunk_size=1000 is provided; langchain will not cut the characters at 1000; it will try to create meaningful chunks near to 1000.   \n",
    "Also each chunk will have some overlap to logically connect the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks=text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeeb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da7665",
   "metadata": {},
   "source": [
    "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
    "\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
    "\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
    "\n",
    "### Sidenote\n",
    "\n",
    "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many document types we have \n",
    "doc_types=set(chunk.metadata[\"doc_type\"] for chunk in chunks)\n",
    "print(doc_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b0a32",
   "metadata": {},
   "source": [
    "### Using OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b653583",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings=OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the datastore if exists already\n",
    "if os.path.exists(db_name_openai):\n",
    "    Chroma(persist_directory=db_name_openai, embedding_function=openai_embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Chroma vectorstore!\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=openai_embeddings, persist_directory=db_name_openai)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7050f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the collection name \n",
    "collection = vectorstore._collection\n",
    "\n",
    "# get one document from the vector store \n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "\n",
    "# get the dimension of document retrieved \n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a787e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133eeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 10 values \n",
    "sample_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47e982",
   "metadata": {},
   "source": [
    "### Visualize the vector store for Open AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27cd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "# get all documents from the vector store \n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "\n",
    "# get vector embeddings in numpy array\n",
    "vectors = np.array(result['embeddings'])\n",
    "\n",
    "# retrieve documents from the result set \n",
    "documents = result['documents']\n",
    "\n",
    "# get the document type from metadata \n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "\n",
    "# set color for each type; blue for products, green for employees and so on \n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d669311",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data in plotly 2D using projection technique \n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84383fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize this in 3D \n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665efd9",
   "metadata": {},
   "source": [
    "# Let's try with local model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a319b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(db_name_ollama):\n",
    "    Chroma(persist_directory=db_name_ollama, embedding_function=ollama_embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9483be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=ollama_embeddings, persist_directory=db_name_ollama)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the collection name \n",
    "collection = vectorstore._collection\n",
    "\n",
    "# get one document from the vector store \n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "\n",
    "# get the dimension of document retrieved \n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 10 values \n",
    "sample_embedding[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f62340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "# get all documents from the vector store \n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "\n",
    "# get vector embeddings in numpy array\n",
    "vectors = np.array(result['embeddings'])\n",
    "\n",
    "# retrieve documents from the result set \n",
    "documents = result['documents']\n",
    "\n",
    "# get the document type from metadata \n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "\n",
    "# set color for each type; blue for products, green for employees and so on \n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data in plotly 2D using projection technique \n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization - Local Model',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize this in 3D \n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5908fad9",
   "metadata": {},
   "source": [
    "# Using these vectors efficiently in our prompt\n",
    "\n",
    "First run this in a cell: `!pip install langchain-ollama`\n",
    "\n",
    "Then replace `llm = ChatOpenAI(temperature=0.7, model_name=MODEL)` with:\n",
    "\n",
    "```python\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(temperature=0.7, model=\"llama3.2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ddc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new chat with ollama \n",
    "llm=ChatOllama(temperature=0.7, model=OLLAMA_MODEL)\n",
    "\n",
    "# set up conversation memory for the chat \n",
    "memory=Con"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
